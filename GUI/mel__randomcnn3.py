# -*- coding: utf-8 -*-
# transfers style from all audios in style folder, linear specs, 2 layers
"""MEL_ randomCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yddr-P_LqW8UVuXpjtUC_f2JHcGPUO9E
"""

from torch.autograd import Variable
from librosa.feature.inverse import mel_to_audio
from scipy.io.wavfile import write

import torch.nn as nn
import time
import math
import librosa
import numpy as np
import torch
import os

from utils2 import *

def imp():
    print("*IMPORTED*")
    style_param = 1
    content_param = 1e4 
    learning_rate = 0.002

    num_epochs = 10000
    print_every = 1000

    cuda = True if torch.cuda.is_available() else False
    print("*GLOBALS SET*")
    
    styles = []

    basepath = "data/"
    
    result_path = basepath + "result/title_G"

    content_source_name = "Base"
    CONTENT_FILENAME = basepath + content_source_name + "/title.wav"

    style_source_name = "recorded/"
    for i in os.listdir(basepath + style_source_name):
        STYLE_FILENAME = basepath + style_source_name + i
        styles.append(wav2spectrum2(STYLE_FILENAME)[0])
        
    a_content, sr = wav2spectrum2(CONTENT_FILENAME)
    a_content_torch = torch.from_numpy(a_content)[None, None, :, :]
    
    if cuda:
        a_content_torch = a_content_torch.cuda()
    print(a_content_torch.shape)
    
    a_style_torch = [torch.from_numpy(i)[None, None, :, :]  for i in styles]


    if cuda:
        a_style_torch = [i.cuda() for i in a_style_torch]
    
    model = RandomCNN()
    model.eval()

    a_C_var = Variable(a_content_torch, requires_grad=False).float()
    a_S_var = [Variable(i, requires_grad=False).float() for i in a_style_torch]

    if cuda:
        model = model.cuda()
        a_C_var = a_C_var.cuda()
        a_S_var = [i.cuda() for i in a_S_var]

    a_C = model(a_C_var)
    a_S = [model(i) for i in a_S_var]

    if cuda:
        a_G_var = Variable(torch.randn(a_content_torch.shape).cuda() * 1e-3, requires_grad=True)
    else:
        a_G_var = Variable(torch.randn(a_content_torch.shape) * 1e-3, requires_grad=True)
    optimizer = torch.optim.Adam([a_G_var])

    start = time.time()
#     Train the Model
    print ("*TRAINING STARTED*")
    for epoch in range(0, num_epochs + 1):
        optimizer.zero_grad()
        a_G = model(a_G_var)

        content_loss = content_param * compute_content_loss(a_C, a_G, False)
        style_loss = sum([style_param * compute_layer_style_loss(i, a_G, False) for i in a_S])
        loss = content_loss + style_loss 
        
        loss.backward()
        optimizer.step()
        

    gen_spectrum = a_G_var.cpu().data.numpy().squeeze()
    gen_audio_C = result_path + '.wav'
    spectrum2wav(gen_spectrum, sr, gen_audio_C)

    print("_________________________________")
